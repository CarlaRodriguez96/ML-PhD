{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c08c7d1",
   "metadata": {},
   "source": [
    "## Neural Networks: \n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#\n",
    "\n",
    "- Optimized gradient descent algorithms \n",
    "- Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99102a",
   "metadata": {},
   "source": [
    "## How to speed-up stochastic gradient descent?\n",
    "\n",
    "- Accelerate (\"momentum\") towards minimum\n",
    "- Automatically choose learning rate\n",
    "- Different rates for different weights\n",
    "\n",
    "**Few gradient update methods:** \n",
    "\n",
    "*Ref: An overview of gradient descent optimization algorithms - Sebastian Ruder (https://arxiv.org/abs/1609.04747)*\n",
    "\n",
    "    1. adagrad - divide by RMS of all previous gradients\n",
    "    2. RMSprop - divide by RMS of last few previous gradients \n",
    "    3. adadelta - same, but multiply also by RMS of last few parameters updates\n",
    "    4. ADAM - divide running avg of last few gradients by RMS of last few gradients (with corrections during earliest steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa83820",
   "metadata": {},
   "source": [
    "### 1. adagrad: (adaptive gradient)\n",
    "\n",
    "Gradient: $\\theta_j$ one of the many parameters (weights) of the net; $g_j$ j-component of the gradient $g$.\n",
    "\\begin{equation}\n",
    "g_j=\\frac{\\partial C}{\\partial \\theta_j} \n",
    "\\end{equation}\n",
    "\n",
    "Standard: I change the vector of parameters $\\theta$ by taking the *old* vector and subtracting an amount of $\\eta$ (learning rate) times the *old* gradient. \n",
    "\\begin{equation}\n",
    "\\frac{d\\theta}{dt}=-\\eta g \\qquad \\qquad discrete \\quad steps: \\qquad \\theta^{(t+1)}=\\theta^{(t)}-\\eta g^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "We don't know how large/small will be the gradient -> treat the gradients independently. \n",
    "\n",
    "Idea: rescale according to estimate of \"typical size\" of $g$; to do so, square the gradient and sum over all the previous time-steps to get an estimate of how large it is and RMS -$\\epsilon$ to avoid division by zero-.\n",
    "\\begin{equation}\n",
    "\\Delta \\theta_j=-\\eta \\cdot \\frac{g_j^{(t)}}{\\sqrt{\\sum_{t'\\leq t}(g_j^{(t)})^2+\\epsilon}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44608d7",
   "metadata": {},
   "source": [
    "### 2. RMSprop: \n",
    "\n",
    "Problem: the sum of $g$ in the *adagrad* denominator will grow over time -> **decay of learning rate**. \n",
    "\n",
    "Solution: Only take RMS of the last few gradients. \n",
    "\n",
    "\\begin{equation}\n",
    "V^{(t)}=\\gamma V^{(t-1)}+(1-\\gamma)(g^{(t)})^2 \\qquad \\rightarrow \\qquad  V^{(t)}=(1-\\gamma)\\sum_{t'\\leq t}\\gamma^{t-t'}(g^{(t')})^2\n",
    "\\end{equation}\n",
    "\n",
    "$V$ = estimate of the variance, being $\\gamma$ a decay term (e.g., $\\approx 0.9$). A way to accumulate the average but also fade away the old: the influence of the old estimates fades away ($\\gamma V^{(t-1)})$. \n",
    "\n",
    "Contributions from $\\approx 1/(1-\\gamma)$ last terms e.g., for time-independent $g^{(t)}=g$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{t'\\leq t}\\gamma^{t-t'}\\approx \\frac{1}{1-\\gamma} \\qquad \\qquad V^{(t)}=(1-\\gamma)\\sum_{t'\\leq t}\\gamma^{t-t'}g^{2}= g^2\n",
    "\\end{equation}\n",
    "\n",
    "Update:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\theta_j^{(t)}=-\\eta \\cdot \\frac{g_j^{(t)}}{\\sqrt{V_j^{(t)}+\\epsilon}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49e185",
   "metadata": {},
   "source": [
    "### 3. adadelta: \n",
    "\n",
    "Multiply also by RMS of last few parameter updates: \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{V}^{(t)}=\\gamma \\hat{V}^{(t-1)}+(1-\\gamma)(\\Delta \\theta^{(t)})^2\n",
    "\\end{equation}\n",
    "\n",
    "No learning rate $\\eta$\n",
    "\\begin{equation}\n",
    "\\Delta \\theta^{(t)}=-\\frac{\\hat{V}^{(t-1)}}{\\hat{V}^{(t)}}g^{(t)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d9e2a",
   "metadata": {},
   "source": [
    "### 4. adam: \n",
    "\n",
    "Divide running average of last few gradients by RMS of last few gradients (with corrections during earliest steps):\n",
    "\n",
    "\\begin{equation}\n",
    "n^{(t)}=\\beta m^{(t-1)}+ (1-\\beta)g^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "being $g^{(t)}$ the actual value of the gradient, $\\beta<1$, $m^{(t-1)}$ old value of the *mean* gradient, then $n^{(t)}$ isn't the present value of the gradient but something like an avg. over the last few $g's$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{V}^{(t)}=\\gamma \\hat{V}^{(t-1)}+(1-\\gamma)(\\Delta \\theta^{(t)})^2\n",
    "\\end{equation}\n",
    "\n",
    "Little problem: $m^{(t)}\\approx 0$,  $V^{(t)}\\approx 0$ in first steps. Correct via:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{m}^{(t)}=\\frac{m^{(t)}}{1-\\beta^t}\\qquad \\qquad \\hat{V}^{(t)}=\\frac{V^{(t)}}{1-\\gamma^t}\n",
    "\\end{equation}\n",
    "\n",
    "Step:\n",
    "\\begin{equation}\n",
    "\\Delta \\theta^{(t)}=-\\eta \\cdot \\frac{\\hat{m}^{(t)}}{\\sqrt{\\hat{V}^{(t)}+\\epsilon}} \\qquad \\qquad \\beta\\approx 0.9, \\quad \\gamma\\approx 0.999, \\quad \\epsilon\\approx 10^{-8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496417cb",
   "metadata": {},
   "source": [
    "## Recurrent neural networks:\n",
    "\n",
    "- Networks with \"memory\". \n",
    "- Useful for \n",
    "            \n",
    "        Analyzing time-evolution (time-series of data)\n",
    "        Analyzing and translating sentences\n",
    "        For control/feedback (robotics or games)\n",
    "        ...\n",
    "\n",
    "*Use a convolutional NN: so that we input a time-series (filter size = memory time) and it outputs a modifyed time series. Are OK for problems in local time / short memory.*\n",
    "       \n",
    "       But long memories with CNN are challenging: need larger filter sizes, may need to know required memory time beforehand, can expand memory time efficiently by multi-layer network with subsampling (pooling), but this is problematic for precise long-term memory.  \n",
    "       \n",
    "**Recurrent neural networks (RNN)**:\n",
    "\n",
    "(1) Unfolding the inputs in time: each circle is represented in a certain time-step. \n",
    "\n",
    "(2) Outputs at given time not only depend on their input at same time. \n",
    "\n",
    "(3) Outputs aren't transmitted to the outside but we keep part of those to be available in the next time-step -> things like, for a given circle $i$, taking both the input ($i$) and the previous output ($i-1$) in order to calculate the output ($i$) of the circle (which can be a neuron or a layer).  \n",
    "\n",
    "(4) Weights (strenght of the arrows) are *time independent*!\n",
    "\n",
    "       output:  o -> o -> o -> o - (keep memory) -> o -> o -> o -> ...\n",
    "                ^    ^    ^    ^                    ^    ^    ^ \n",
    "                |    |    |    |                    |    |    |\n",
    "       input:   o    o    o    o                    o    o    o \n",
    "       --------------------------------------------------------------> time\n",
    "        \n",
    "Advantage: in principle this could give arbitrarily long memory. Each circle $o$ may represent multiple neurons (i.e., layer). Each arrow represents all possible connections between those neurons. \n",
    "\n",
    "        - Train on many input sequences \n",
    "        - Tell the net what would have been the correct output for this particular sequence\n",
    "        - Try to adapt my weights in order to make this happen\n",
    "\n",
    "Training process:\n",
    "\n",
    "       output:  o -> o -> o -> o - (keep memory) -> o -> o -> o    <- Correct answer is known here; take the gradient of\n",
    "                ^    ^    ^    ^                    ^    ^    ^       the cost function with respect to this last output \n",
    "                |    |    |    |                    |    |    |       but then we get gradients with everything that is \n",
    "       hidden:  o -> o -> o -> o -     ...       -> o -> o -> o       connected to this output; need to go backward in time:\n",
    "                ^    ^    ^    ^                    ^    ^    ^       BACKPROPAGATION GOES DOWN IN THE LAYERS BUT ALSO \n",
    "                |    |    |    |                    |    |    |       BACK IN TIME\n",
    "       input:   o    o    o    o       ...          o    o    o \n",
    "       ---------------------------------------------------------> time\n",
    "                                                                <----  BACKPROPAGATION THROUGH TIME\n",
    "                                                                    |\n",
    "                                                                    v\n",
    "      \n",
    "**Challenge**: long memories with RNN are challenging due to a feature of backpropagation -> \"*Exploding gradients*\" / \"*Vanishing gradients*\" -> Backpropagation through many layers (deep net) or many time-steps (RNN): \n",
    "\n",
    "Each step in the backpropagation is a matrix-vector multiplication, deviation vector may be something like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta_{t-1}=M_t\\Delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Depending on the typical eigenvalues of the matrices $M$, $|\\Delta|$ can explode (regions where eigenvalues are large and so the deviation vector keeps growing) or vanish (regions where eigenvalues are small and so the deviation vector vanishes).\n",
    "\n",
    "**Solution: Long short-term memory (LSTM)**\n",
    "\n",
    "    Long-term memory = weights that are adapted during training and stored forever\n",
    "    Short-term memory = input-dependent memory\n",
    "    \n",
    "*LSTM tries to have long memory times in a robust way for this short-time memory*: Determine read/write/delete operations of a memory cell via the network (through other neurons). Most of the time, a memory neuron just sits there and isn't used or changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3ba0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
