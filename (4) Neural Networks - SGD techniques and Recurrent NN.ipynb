{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c08c7d1",
   "metadata": {},
   "source": [
    "## Neural Networks: \n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#\n",
    "\n",
    "- Optimized gradient descent algorithms \n",
    "- Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99102a",
   "metadata": {},
   "source": [
    "## How to speed-up stochastic gradient descent?\n",
    "\n",
    "- Accelerate (\"momentum\") towards minimum\n",
    "- Automatically choose learning rate\n",
    "- Different rates for different weights\n",
    "\n",
    "**Few gradient update methods:** \n",
    "\n",
    "*Ref: An overview of gradient descent optimization algorithms - Sebastian Ruder (https://arxiv.org/abs/1609.04747)*\n",
    "\n",
    "    1. adagrad - divide by RMS of all previous gradients\n",
    "    2. RMSprop - divide by RMS of last few previous gradients \n",
    "    3. adadelta - same, but multiply also by RMS of last few parameters updates\n",
    "    4. ADAM - divide running avg of last few gradients by RMS of last few gradients (with corrections during earliest steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa83820",
   "metadata": {},
   "source": [
    "### 1. adagrad: (adaptive gradient)\n",
    "\n",
    "Gradient: $\\theta_j$ one of the many parameters (weights) of the net; $g_j$ j-component of the gradient $g$.\n",
    "\\begin{equation}\n",
    "g_j=\\frac{\\partial C}{\\partial \\theta_j} \n",
    "\\end{equation}\n",
    "\n",
    "Standard: I change the vector of parameters $\\theta$ by taking the *old* vector and subtracting an amount of $\\eta$ (learning rate) times the *old* gradient. \n",
    "\\begin{equation}\n",
    "\\frac{d\\theta}{dt}=-\\eta g \\qquad \\qquad discrete \\quad steps: \\qquad \\theta^{(t+1)}=\\theta^{(t)}-\\eta g^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "We don't know how large/small will be the gradient -> treat the gradients independently. \n",
    "\n",
    "Idea: rescale according to estimate of \"typical size\" of $g$; to do so, square the gradient and sum over all the previous time-steps to get an estimate of how large it is and RMS -$\\epsilon$ to avoid division by zero-.\n",
    "\\begin{equation}\n",
    "\\Delta \\theta_j=-\\eta \\cdot \\frac{g_j^{(t)}}{\\sqrt{\\sum_{t'\\leq t}(g_j^{(t)})^2+\\epsilon}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44608d7",
   "metadata": {},
   "source": [
    "### 2. RMSprop: \n",
    "\n",
    "Problem: the sum of $g$ in the *adagrad* denominator will grow over time -> **decay of learning rate**. \n",
    "\n",
    "Solution: Only take RMS of the last few gradients. \n",
    "\n",
    "\\begin{equation}\n",
    "V^{(t)}=\\gamma V^{(t-1)}+(1-\\gamma)(g^{(t)})^2 \\qquad \\rightarrow \\qquad  V^{(t)}=(1-\\gamma)\\sum_{t'\\leq t}\\gamma^{t-t'}(g^{(t')})^2\n",
    "\\end{equation}\n",
    "\n",
    "$V$ = estimate of the variance, being $\\gamma$ a decay term (e.g., $\\approx 0.9$). A way to accumulate the average but also fade away the old: the influence of the old estimates fades away ($\\gamma V^{(t-1)})$. \n",
    "\n",
    ">Contributions from $\\approx 1/(1-\\gamma)$ last terms e.g., for time-independent $g^{(t)}=g$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{t'\\leq t}\\gamma^{t-t'}\\approx \\frac{1}{1-\\gamma} \\qquad \\qquad V^{(t)}=(1-\\gamma)\\sum_{t'\\leq t}\\gamma^{t-t'}g^{2}= g^2\n",
    "\\end{equation}\n",
    "\n",
    ">Update:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\theta_j^{(t)}=-\\eta \\cdot \\frac{g_j^{(t)}}{\\sqrt{V_j^{(t)}+\\epsilon}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49e185",
   "metadata": {},
   "source": [
    "### 3. adadelta: \n",
    "\n",
    "Multiply also by RMS of last few parameter updates: \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{V}^{(t)}=\\gamma \\hat{V}^{(t-1)}+(1-\\gamma)(\\Delta \\theta^{(t)})^2\n",
    "\\end{equation}\n",
    "\n",
    "No learning rate $\\eta$\n",
    "\\begin{equation}\n",
    "\\Delta \\theta^{(t)}=-\\frac{\\hat{V}^{(t-1)}}{\\hat{V}^{(t)}}g^{(t)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d9e2a",
   "metadata": {},
   "source": [
    "### 4. adam: \n",
    "\n",
    "Divide running average of last few gradients by RMS of last few gradients (with corrections during earliest steps):\n",
    "\n",
    "\\begin{equation}\n",
    "n^{(t)}=\\beta m^{(t-1)}+ (1-\\beta)g^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "being $g^{(t)}$ the actual value of the gradient, $\\beta<1$, $m^{(t-1)}$ old value of the *mean* gradient, then $n^{(t)}$ isn't the present value of the gradient but something like an avg. over the last few $g's$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{V}^{(t)}=\\gamma \\hat{V}^{(t-1)}+(1-\\gamma)(\\Delta \\theta^{(t)})^2\n",
    "\\end{equation}\n",
    "\n",
    "Little problem: $m^{(t)}\\approx 0$,  $V^{(t)}\\approx 0$ in first steps. Correct via:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{m}^{(t)}=\\frac{m^{(t)}}{1-\\beta^t}\\qquad \\qquad \\hat{V}^{(t)}=\\frac{V^{(t)}}{1-\\gamma^t}\n",
    "\\end{equation}\n",
    "\n",
    ">Step:\n",
    "\\begin{equation}\n",
    "\\Delta \\theta^{(t)}=-\\eta \\cdot \\frac{\\hat{m}^{(t)}}{\\sqrt{\\hat{V}^{(t)}+\\epsilon}} \\qquad \\qquad \\beta\\approx 0.9, \\quad \\gamma\\approx 0.999, \\quad \\epsilon\\approx 10^{-8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496417cb",
   "metadata": {},
   "source": [
    "## Recurrent neural networks:\n",
    "\n",
    "- Networks with \"memory\". \n",
    "- Useful for: \n",
    "            \n",
    "        Analyzing time-evolution (time-series of data)\n",
    "        Analyzing and translating sentences\n",
    "        For control/feedback (robotics or games)\n",
    "        ...\n",
    "\n",
    "*Use a convolutional NN: so that we input a time-series (filter size = memory time) and it outputs a modifyed time series. Are OK for problems in local time / short memory.*\n",
    "       \n",
    "       But long memories with CNN are challenging: need larger filter sizes, may need to know required memory time \n",
    "       beforehand, can expand memory time efficiently by multi-layer network with subsampling (pooling), \n",
    "       but this is problematic for precise long-term memory.  \n",
    "       \n",
    "**Recurrent neural networks (RNN)**:\n",
    "\n",
    "(1) Unfolding the inputs in time: each circle is represented in a certain time-step. \n",
    "\n",
    "(2) Outputs at given time not only depend on their input at same time. \n",
    "\n",
    "(3) Outputs aren't transmitted to the outside but we keep part of those to be available in the next time-step -> things like, for a given circle $i$, taking both the input ($i$) and the previous output ($i-1$) in order to calculate the output ($i$) of the circle (which can be a neuron or a layer).  \n",
    "\n",
    "(4) Weights (strenght of the arrows) are *time independent*!\n",
    "\n",
    "       output:  o -> o -> o -> o - (keep memory) -> o -> o -> o -> ...\n",
    "                ^    ^    ^    ^                    ^    ^    ^ \n",
    "                |    |    |    |                    |    |    |\n",
    "       input:   o    o    o    o                    o    o    o \n",
    "       --------------------------------------------------------------> time\n",
    "        \n",
    ">Advantage: in principle this could give arbitrarily long memory. Each circle $o$ may represent multiple neurons (i.e., layer). Each arrow represents all possible connections between those neurons. \n",
    "\n",
    "        - Train on many input sequences \n",
    "        - Tell the net what would have been the correct output for this particular sequence\n",
    "        - Try to adapt my weights in order to make this happen\n",
    "\n",
    "Training process:\n",
    "\n",
    "       output:  o -> o -> o -> o - (keep memory) -> o -> o -> o    <- Correct answer is known here; take the gradient of\n",
    "                ^    ^    ^    ^                    ^    ^    ^       the cost function with respect to this last output \n",
    "                |    |    |    |                    |    |    |       but then we get gradients with everything that is \n",
    "       hidden:  o -> o -> o -> o -     ...       -> o -> o -> o       connected to this output; need to go backward in time:\n",
    "                ^    ^    ^    ^                    ^    ^    ^       BACKPROPAGATION GOES DOWN IN THE LAYERS BUT ALSO \n",
    "                |    |    |    |                    |    |    |       BACK IN TIME\n",
    "       input:   o    o    o    o       ...          o    o    o \n",
    "       ---------------------------------------------------------> time\n",
    "                                                                <----  BACKPROPAGATION THROUGH TIME\n",
    "                                                                    |\n",
    "                                                                    v\n",
    "      \n",
    ">**Challenge**: long memories with RNN are challenging due to a feature of backpropagation -> \"*Exploding gradients*\" / \"*Vanishing gradients*\" -> Backpropagation through many layers (deep net) or many time-steps (RNN): \n",
    "\n",
    "Each step in the backpropagation is a matrix-vector multiplication, deviation vector may be something like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta_{t-1}=M_t\\Delta_t\n",
    "\\end{equation}\n",
    "\n",
    "Depending on the typical eigenvalues of the matrices $M$, $|\\Delta|$ can explode (regions where eigenvalues are large and so the deviation vector keeps growing) or vanish (regions where eigenvalues are small and so the deviation vector vanishes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdfb8d",
   "metadata": {},
   "source": [
    "**Solution: Long short-term memory (LSTM)**\n",
    "\n",
    "    Long-term memory = weights that are adapted during training and stored forever\n",
    "    Short-term memory = input-dependent memory\n",
    "    \n",
    "*LSTM tries to have long memory times in a robust way for this short-time memory*: Determine read/write/delete operations of a memory cell via the network (through other neurons). Most of the time, a memory neuron just sits there and isn't used or changed.\n",
    "\n",
    "**A. Deleting the memory - LSTM: Forget date**\n",
    "  \n",
    "                               c_(t-1)          c_t\n",
    "    memory cell content, c:    o -------x-----> o\n",
    "    time-step from t-1 to t             |   \n",
    "                                    keep:  c_t = 1 x c_(t-1)\n",
    "                                  delete:  c_t = 0 x c_(t-1)\n",
    "                                  \n",
    "Multiply the input $c_{t-1}$ by 1 (or 0) if we want to keep it (or delete it). We want these 1 or 0 to be calculated based on the values of some other neurons. \n",
    "\n",
    "                               c_(t-1)          c_t\n",
    "    memory cell content, c:    o -------x-----> o\n",
    "    time-step from t-1 to t             ^\n",
    "                                        |   \n",
    "                                 (forget gate, f)\n",
    "                                        |\n",
    "                           input, x_t   o \n",
    "                           \n",
    "Calculate the *forget gate*: value $f$ which may be 1, or 0 or something in-between. \n",
    "\n",
    "\\begin{equation}\n",
    "f=\\sigma(W^{(f)}x_t+b^{(f)})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is **sigmoid**, $x$, $b$ and $f$ are vectors and $W$ the weight matrix. Now obtain new memory content: **we are multiplying neuron values**\n",
    "\n",
    "\\begin{equation}\n",
    "c_t=f \\star c_{t-1}, \\qquad \\star=elementwise\\quad product\n",
    "\\end{equation}\n",
    "\n",
    "Product rule for time $t$: the multiplication $\\star$ splits the error backpropagation into two branches. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f_j c_{t-1,j}}{\\partial w_\\star} = \\frac{\\partial f_j}{\\partial w_\\star}c_{t-1,j} +  f_j\\frac{\\partial c_{t-1,j}}{\\partial w_\\star}\n",
    "\\end{equation}\n",
    "\n",
    ">Whole picture: \n",
    "\n",
    "                               c_(t-2)          c_(t-1)          c_(t)           c_(t+1)\n",
    "    memory cell content, c:      o -------x-----> o -------x-----> o -------x-----> o \n",
    "    time-step from t-1 to t+1             ^                ^                ^ \n",
    "                                          |                |                |\n",
    "                                   (forget gate, f) (forget gate, f) (forget gate, f)\n",
    "                                          |                |                |\n",
    "                        input:  x_(t-1)   o          x_t   o       x_(t+1)  o\n",
    "\n",
    "**B. Writing new memory - LSTM**\n",
    "\n",
    "                               c_(t-1)            c_t\n",
    "    memory cell content, c:    o ------- + -----> o\n",
    "                                         ^\n",
    "                                         |\n",
    "                                         x <-- new value, c'_t\n",
    "                                         ^          |\n",
    "                                         |          |\n",
    "                                 (input gate, i)    |\n",
    "                                         |          |\n",
    "                           input, x_t    o ---------|\n",
    "\n",
    ">$i=\\sigma(W^{(i)}x_t+b^{(i)})$ and $c'_t=tanh(W^{(c)}x_t+b^{(c)})$;\n",
    "\n",
    "**Both delete & write together**: $c_t=f*c_{t-1} + i*c'_t$\n",
    "\n",
    "**C. Read (output) memory value - LSTM**\n",
    "               \n",
    "                                                  ^ \n",
    "                                                  | h_t\n",
    "                                                  x <-----|\n",
    "                               c_(t-1)            |       |\n",
    "    memory cell content, c:    o ------- + -----> o  c_t  |\n",
    "                                                          |\n",
    "                                                          |\n",
    "                                                  (output gate, o)\n",
    "                                                          ^\n",
    "                                                          |\n",
    "                                            input, x_t    o\n",
    "\n",
    ">$o=\\sigma(W^{(o)}x_t+b^{(i)})$ and $h_t=o*tanh(c_t)$\n",
    "\n",
    "**D. LSTM: exploit previous memory output 'h'**\n",
    "\n",
    "Make $f$, $i$, $o$, etc. at time $t$ depend on output 'h' calculated in previous time step. Otherwise, 'h' could only be used in higher layers, but not to control memory access in present layer\n",
    "\n",
    "\\begin{equation}\n",
    "f=\\sigma(W^{(f)}x_t+U^{(f)}h_{t-1}+b^{(f)})\n",
    "\\end{equation}\n",
    "\n",
    "likewise for every other quantity. Thus, result of readout can actually influence subsequent operations. Sometimes 'o' is even made to depend on $c_t$. \n",
    "\n",
    "Coming back to backpropagation: during those 'silent' time-intervals, there's NO explosion or vanishing gradient. \n",
    "\n",
    "\\begin{equation}\n",
    "c_t=c_{t-1}=c_{t-2}=... \\qquad \\qquad \\frac{\\partial c_t}{\\partial w_\\star}=\\frac{\\partial c_{t-1}}{\\partial w_\\star}=\\frac{\\partial c_{t-2}}{\\partial w_\\star}=...\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an LSTM layer with 10 memory cells: each of those cells has the full structure, \n",
    "# with f, i, o gates and the memory content c, and the output h. \n",
    "rnn.add(LSTM(10, return_sequences=True)) # Whether to return the full-time sequence of outputs or only the final time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be5b44",
   "metadata": {},
   "source": [
    "Full application: take an input of 3 neuron values for each time step and producing a time sequence with 2 neuron values for each time step.\n",
    "\n",
    "                            LSTM ( o o ) 2, output \n",
    "                                    ^\n",
    "                                    |\n",
    "                         LSTM ( o o o o o ) 5, hidden\n",
    "                                    ^\n",
    "                                    |      \n",
    "                                ( o o o ) 3, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory_net():\n",
    "    global rnn, batchsize, timesteps\n",
    "    rnn = Sequential()\n",
    "    \n",
    "    rnn.add(LSTM(5, batch_input_shape=(None, timesteps, 3), return_sequences=True))\n",
    "    rnn.add(LSTM(2, return_sequences=True))\n",
    "    rnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# (!!) batch_input_shape is (batchsize, timesteps, data_dim) -> batch_input_shape=(None, timesteps, 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defffc0",
   "metadata": {},
   "source": [
    "**Recurrent neural networks (RNN): Character generation**\n",
    "\n",
    "Timetraces is a sequence of characters; given a certain piece of text, can you predict what's the likelihood to have one or the other letter as the next letter of the sequence. \n",
    "\n",
    "Input sequence (each letter in one-hot encoder - one letter = one input neuron):\n",
    "        \n",
    "        T H E _ T H E O R Y _ O F _ G E \n",
    "        ------------------------------> time\n",
    "\n",
    "Desired output: predict next character:\n",
    "\n",
    "        H E _ T H E O R Y _ O F _ G E N\n",
    "        ------------------------------> time\n",
    "\n",
    "Network will output probability for **each** possible character at each time step. \n",
    "\n",
    "                       input  = one-hot \n",
    "                       output = probability distirbution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe765da",
   "metadata": {},
   "source": [
    "**Recurrent neural networks (RNN): Word vectors**\n",
    "\n",
    "Simple one-hot encoding of words needs large vectors; dimension = number of words in dictionary. \n",
    "\n",
    "    Input neurons: each of them corresponds to one single word. \n",
    "    \n",
    "    Compress the words in vectors -> and place the vectors in a high-dim. space in such a way that there's a relation \n",
    "    between them (e.g., word2vec -> \"warm\", \"cold\" and \"hot\" closer and far from \"tree\"). \n",
    "    \n",
    "Noise-contrastive estimation: provide few noisy (wrong) examples and train the model to predict that they are fake but that the true one is correct. \n",
    "\n",
    "    (1) Give the context words and make it guess the word - 'continuous bag of words'\n",
    "    (2) Give the word and make it guess the context words - 'skip-gram'\n",
    "\n",
    "Model tries to predict:\n",
    "\n",
    "$P_\\theta(w,h)$, probability that $w$ is the correct word given the context word $h$, $\\theta$ are the parameters of the model (weights, biases and entries of embedding vectors). \n",
    "\n",
    "$P_\\theta(w,h)=\\sigma(W_{jk}e_{k}(h)+b_j)$, $j$, index for word w in dictionart, $k$ index in embedding vector (Einstein sum), $e(h)$ embedding vector for word h, $W,b$ weights and biases. \n",
    "\n",
    "At each time-step: go down the gradient of $C=-(lnP_\\theta(w_t,h)+\\sum_{\\hat{w}}ln(1-P_\\theta(\\hat{w},h)))$, being $\\hat{w}$ noisy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc44a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer for mapping word indices to word vectors, for input sequences of some given length\n",
    "\n",
    "embedding_layer=Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Helper routines @ Keras: \n",
    "# Tokenizer (Text preprocessing)\n",
    "# pad_sequences (Sequences preprocessing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
