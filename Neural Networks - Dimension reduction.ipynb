{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627c186f",
   "metadata": {},
   "source": [
    "## Neural Networks: \n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#\n",
    "\n",
    "- Dimensionality-reducing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5add3",
   "metadata": {},
   "source": [
    "## Visualizing high-dimensional data: return to unsupervised & non-linear ML\n",
    "\n",
    "If we want to represent/visualize some of the neuron values in some intermediate layer (but the're so many of them...) -> need to project/reduce down to **2 dimensions** -> smart ways to project -> important characteristics I want to keep:  keeping the distance relation quantitatively similar (see which inputs are close to each other / or very different, etc.).\n",
    "\n",
    "    1. Obtain PCA, plot the components of each image with respect to 2 eigenvectors with the LARGEST eigenvalues (the 2 components explaining the largest variance).  \n",
    "    \n",
    "    2. t-SNE: high-dimensional space to 2D - without LABELING! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee74118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
