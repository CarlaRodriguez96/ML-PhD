{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44124bff",
   "metadata": {},
   "source": [
    "## Neural Networks: \n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#\n",
    "\n",
    "- Dimensionality-reducing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a5c6b",
   "metadata": {},
   "source": [
    "## Visualizing high-dimensional data: return to unsupervised & non-linear ML\n",
    "\n",
    "If we want to represent / visualize some of the neuron values in some intermediate layer (but the're so many of them...) -> need to project / reduce down to **2 dimensions** -> smart ways to project -> important characteristics I want to keep:  keeping the distance relation quantitatively similar (see which inputs are close to each other / or very different, etc.).\n",
    "\n",
    "    1. Obtain PCA, plot the components of each image with respect to 2 eigenvectors with the LARGEST eigenvalues (the 2 components explaining the largest variance).\n",
    "    \n",
    "    2. t-SNE: high-dimensional space to 2D - without LABELING!\n",
    "    \n",
    "Cost function that depends on how close the distances of low-dimensional datapoints 'y' are to those of high-dimensional points 'x':\n",
    "\n",
    "\\begin{equation}\n",
    "C=\\sum_{i\\neq j}F(|x_i-x_j|, |y_i-y_j|)\n",
    "\\end{equation}\n",
    "\n",
    "if these two distances $|x_i-x_j|$, $|y_i-y_j|$ are equal, $F$ is minimum (zero), otherwise $F>0$. \n",
    "\n",
    "Minimize the cost function (e.g., SGD): points in low-dim. space **repel if they're closer than their counterparts in high-dim. and attract otherwise**.\n",
    "\n",
    "Time-derivative of a point in the low dimensional space will be minus the gradient of the cost function with respect to this point:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dy_j}{dt}=-\\frac{\\partial C}{\\partial y_j}\n",
    "\\end{equation}\n",
    "\n",
    "- **Stochastic neighbor embedding (SNE)**: define the probability distributions that depend not only on the distance but also incluse some normalization. \n",
    "\n",
    "$p_{ij}$ Probability to pick a pair of points (i,j); defined to be **larger** if they're close neighbors in **high-dim. space**. \n",
    "\n",
    "$q_{ij}$ Probability to pick a pair of points (i,j); defined to be **larger** if they're close neighbors in **low-dim. space**. \n",
    "\n",
    "Normalized: $\\sum_{i \\neq j}q_{ij}=1$; $\\sum_{i \\neq j}p_{ij}=1$;\n",
    "\n",
    "We want **q-distribution** to be a close approximation of the **p-distribution**:\n",
    " \n",
    "                                   Kullback - Leiber divergence: a way of comparing probab. dist.:\n",
    "\\begin{equation}\n",
    "C=KL(P\\|Q)=\\sum_i \\sum_j p_{ij} log\\frac{p_{ij}}{q_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "- **Choices made for t-SNE**: q is comparatively larger at long distances; allows points in low-dim. to spread out for intermediate distances. \n",
    "\n",
    "A. **High-dimensional space (Gaussians distance)**: essentially an exponential of the distance ($\\|x_i-x_j\\|$) squared; falls off for large distances. \n",
    "\\begin{equation}\n",
    "p_{j\\|i}=\\frac{exp(-\\|x_i-x_j\\|^2 2\\sigma_i^2)}{\\sum_{k\\neq i}exp(-\\|x_i-x_k\\|^2 2\\sigma_i^2)}\\qquad \\qquad p_{ij}=\\frac{p_{j\\|i}+p_{i\\|j}}{2n}\n",
    "\\end{equation}\n",
    "\n",
    "B. **Low-dimensional space (Cauchy dist. = Student-t distance)**: essentially the inverse of the square of the distance; this fall off for large distances but not so quickly as the gaussian. \n",
    "\\begin{equation}\n",
    "q_{ij}=\\frac{(1+\\|y_i-y_j\\|^2)^{-1}}{\\sum_{k\\neq l}(1+\\|y_k-y_l\\|^2)^{-1}}\n",
    "\\end{equation}\n",
    "\n",
    "- **The t-SNE force (gradients)**: the sign of $(p_{ij}-q_{ij})$ depends on match between low and high-dim. distributions; $(y_i - y_j)$ spring-like term, very important at low distances -> leads two points - very close at high-dim. space to have a spring-like attraction in low-dim. space; fall-off term: $(1+\\|y_i - y_j\\|^2)^{-1}$ \"gravity-like\" at larger distances. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta C}{\\delta y_i}= 4\\sum_j (p_{ij}-q_{ij})(y_i - y_j)(1+\\|y_i - y_j\\|^2)^{-1}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
