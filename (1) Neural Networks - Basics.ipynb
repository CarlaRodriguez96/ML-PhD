{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da266d54",
   "metadata": {},
   "source": [
    "## Neural Networks: basics - smol guide & examples\n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#\n",
    "\n",
    "- Basic structure of NN\n",
    "- Cost function and SGD \n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5ad27",
   "metadata": {},
   "source": [
    "## Basics:\n",
    "\n",
    "- Non-linear function of many variables that depends on many parameters.\n",
    "\n",
    "        Single neuron: \n",
    "        \n",
    "        input values = y1,...,yj  ->  output value = f(z)\n",
    "        \n",
    ">The output, f(z), of a neuron is a non-linear function of the weighted sum of inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\sum_j w_j y_j + b\n",
    "\\end{equation}\n",
    "\n",
    "*We feed a non-linear function f(z) with the weighted sum z so that the value assigned to that neuron is f(z)*.\n",
    "\n",
    "- $f()$ can be the same for all neurons in NN (smooth-step functions -> take gradient)\n",
    "    - sigmoid,\n",
    "    - rectified linear unit,\n",
    "    - ...\n",
    "\n",
    "- Elementary building blocks:\n",
    "    - Each connection has a weight $w$\n",
    "    - Each neuron has an offset (bias) $b$\n",
    "    - Each neuron has a non-linear function $f()$ fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe13eb6",
   "metadata": {},
   "source": [
    "## Feedforward:\n",
    "\n",
    "Evaluation of the NN layer by layer - calculate the output from the input. \n",
    "\n",
    "            o (value = 0.1) --- (w1=0.5) ---->\n",
    "            o (value = 1.3) --- (w2=0.1) ---->  f(z)   <- z = 0.5*0.1 + 0.1*1.3 + 0.5*0.1\n",
    "                            ...  ......   ....\n",
    "            o (value = 0.5) --- (wn=0.1) ---->\n",
    "            \n",
    "* One layer: \n",
    "\n",
    ">Output sum: \n",
    "\n",
    "\\begin{equation}\n",
    "z_j = \\sum_k w_{jk} y_{k}^{in} + b_j, \\qquad elementwise\\quad non-linear\\quad function: \\qquad y_j^{out}=f(z_j)\n",
    "\\end{equation}\n",
    "\n",
    "being $z_j$ the output sum, $j$ the output neuron index, $k$ the input neuron index, $y_{k}^{in}$ the input neuron value and $b_j$ the output bias. \n",
    "\n",
    ">In matrix notation:\n",
    "\n",
    "\\begin{equation}\n",
    "z = wy^{in} + b\n",
    "\\end{equation}\n",
    "\n",
    "being $z$ the output vector, $y^{in}$ the input neuron value (vector), $b_j$ the output bias (vector) and  $w$ the weights matrix. \n",
    "\n",
    "**In python:**\n",
    "\n",
    "    z=dot(w,y)+b\n",
    "                being z = [N out], w = [N out x N in], y = [N in], b = [N out]\n",
    "                    \n",
    "**Define the action of calculating the output neurons from the input as a function**; e.g., for a sigmoid f(z):\n",
    "\n",
    "    def apply_net(y_in):\n",
    "        global w,b\n",
    "    \n",
    "        z=dot(w,y_in)+b\n",
    "        return(1/(1+exp(-z)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1e9da",
   "metadata": {},
   "source": [
    "#### (A) Basic NN (no hidden layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import * \n",
    "\n",
    "N0=3 #Input layer size\n",
    "N1=2 #Output layer size\n",
    "\n",
    "w=random.uniform(low=-1, high=+1, size=(N1,N0)) #Weights\n",
    "b=random.uniform(low=-1, high=+1, size=N1) #Bias\n",
    "\n",
    "y_in=array([0.2, 0.4, -0.1]) #Input layer of values\n",
    "\n",
    "z=dot(w,y_in)+b #Weighted sum of input values\n",
    "\n",
    "y_out=1/(1+exp(-z)) #Sigmoid function (activate function)\n",
    "\n",
    "def apply_net(y_in):\n",
    "    global w,b\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9141ad7",
   "metadata": {},
   "source": [
    "#### (B) Multilayer NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f446eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the operation of a layer\n",
    "def apply_layer(y_in,w,b):  #Weights and biases are no longer global, they change from layer to layer\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))\n",
    "\n",
    "N0=2 #Input layer size\n",
    "N1=30 #Hidden layer size\n",
    "N2=1 #Output layer size\n",
    "\n",
    "#FROM INPUT LAYER TO HIDDEN LAYER:\n",
    "w1=random.uniform(low=-10, high=+10, size=(N1,N0)) #Weights, size is always (output dimension x input dimension)\n",
    "b1=random.uniform(low=-1, high=+1, size=N1) #Bias\n",
    "\n",
    "#FROM HIDDEN LAYER TO OUTPUT LAYER:\n",
    "w2=random.uniform(low=-10, high=+10, size=(N2,N1)) #Weights\n",
    "b2=random.uniform(low=-1, high=+1, size=N2) #Bias\n",
    "\n",
    "#Define the network: succession of layers\n",
    "def apply_net(y_in):\n",
    "    global w1,b1,w2,b2\n",
    "    \n",
    "    y1=apply_layer(y_in,w1,b1)\n",
    "    y2=apply_layer(y1,w2,b2)\n",
    "    return(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad816e",
   "metadata": {},
   "source": [
    "#### (C) NN with many hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed67650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the operation of a layer\n",
    "def apply_layer(y_in,w,b):  #Weights and biases are no longer global, they change from layer to layer\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))\n",
    "\n",
    "Nlayers=20 \n",
    "LayerSize=100\n",
    "\n",
    "Weights=random.uniform(low=-3, high=3, size=[Nlayers,LayerSize,LayerSize])\n",
    "Biases=random.uniform(low=-1, high=1, size=[Nlayers,LayerSize])\n",
    "\n",
    "#First hidden layer (coming in from the input layer)\n",
    "WeightsFirst=random.uniform(low=-1, high=+1, size=[2,LayerSize]) \n",
    "BiasesFirst=random.uniform(low=-1, high=+1, size=LayerSize) \n",
    "\n",
    "#Final layer (output neuron)\n",
    "WeightsFinal=random.uniform(low=-1, high=+1, size=[LayerSize,1]) \n",
    "BiasesFinal=random.uniform(low=-1, high=+1, size=1) \n",
    "\n",
    "#Define the network: succession of layers\n",
    "def apply_multi_net(y_in):\n",
    "    global Weights, Biases, WeightsFinal, BiasesFinal, Nlayers\n",
    "    \n",
    "    y=apply_layer(y_in,WeightsFirst,BiasesFirst)\n",
    "    for j in range(Nlayers):\n",
    "        y=apply_layer(y,Weights[j,:,:],Biases[j,:])\n",
    "        \n",
    "    output=apply_layer(y,WeigthsFinal,BiasesFinal)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e8a50",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent: \n",
    "\n",
    "Make things more efficient by processing batchs (many samples in parallel) -> **expand the arrays**\n",
    "\n",
    "    One sample:  y = [N in]              z = dot(w,y)+b \n",
    "    Many samp.:  y = [N samples x N in]  -> [N samples x N out] = [N samples x N in]*[N in x N out] + [N samples x N out ]\n",
    "\n",
    "- Approximating an arbitrary non-linear function: \n",
    "\n",
    "\\begin{equation}\n",
    "y_{out}= \\delta F_1 f(w(y-Y_1))+\\delta F_2 f(w(y-Y_2))\n",
    "\\end{equation}\n",
    "\n",
    "being $\\delta F_1$ the stephigh, $f()$ a sigmoid: step function of step 1 at location $Y_1$ (and $Y_2$). \n",
    "\n",
    "                    y1\n",
    "               ---> o-----\n",
    "             w |         | delta F1\n",
    "               |         v\n",
    "             y o         o y_out\n",
    "               |         ^\n",
    "             w |         | delta F2\n",
    "               ---> o-----\n",
    "                    y2\n",
    "\n",
    "\\begin{equation}\n",
    "y_1=f(w(y-Y_1)), \\qquad y_2=f(w(y-Y_2)), \\qquad \\qquad  Biases: \\qquad b_1=-wY_1, \\qquad b_2=-wY_2, \n",
    "\\end{equation}\n",
    "\n",
    "- Approximating an arbitrary 2D non-linear function: \n",
    "    - Create quarter-space step function: 'AND' function\n",
    "\\begin{equation}\n",
    "y_{out}= f(w(y_1+y_2 -1.5)) \n",
    "\\end{equation}\n",
    "\n",
    "- **How do we choose the weights and the biases?** We want the NN to produce a smooth curve that goes through all the datapoints as closely as possible. \n",
    "    - NN output $y_{out}$ depends on parameter w -> we want to choose w (adjust) in order to match the points as close as possible.\n",
    "    We have $y_{out}=F_w(y^{in})$, where $F_w$ is the NN with weights w. \n",
    "    We would like $y_{out}\\approx F(y^{in})$, where $F$ is the desired function. \n",
    "\n",
    "**> Cost function**, measures deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "C(w)=\\frac{1}{2}<\\|F_w(y^{in})-F(y^{in})\\|^2>\n",
    "\\end{equation}\n",
    "** averaged overall samples*\n",
    "\n",
    ">For N samples:\n",
    "\n",
    "\\begin{equation}\n",
    "C(w)\\approx \\frac{1}{N} \\frac{1}{2} \\sum_{s=1}^N\\|F_w(y^{s})-F(y^{s})\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "Minimizing C for this case leads to Least Squares fitting;\n",
    "\n",
    "**> General case -> Gradient descent**: $dw/dt\\approx -\\nabla_w C(w)$\n",
    "\n",
    ">*STOCHASTIC GRADIENT DESCENT*: do stochastic sampling (take a small batch of samples and calculate the gradient of C(w)); take different samples at each step. \n",
    "\n",
    "\\begin{equation}\n",
    "w_j \\rightarrow w_j - \\eta \\frac{\\partial \\hat{C}(w)}{\\partial w_j}\n",
    "\\end{equation}\n",
    "\n",
    "$w_j$ parameters (weights and biases), $\\eta$ stepsize (learning rate), $\\hat{C}(w)$ approximate version of $C$.\n",
    "\n",
    "- **For a full NN:**\n",
    "\n",
    "    - $y_j^{(n)}$, neuron $j$ in layer $n$\n",
    "\n",
    "    - $z_j^{(n)}$, input value of $y=f(z)$\n",
    "\n",
    "    - $y_{jk}^{n,n-1}$, weight: neuron $k$ in layer $n-1$ feeding into neuron $j$ in layer $n$\n",
    "    \n",
    "    - $C(w,y^{in})$ the cost value for one particular input neuron\n",
    "    \n",
    "    - $w_\\star$ some weight or bias somewhere in the net\n",
    "    \n",
    "    - $y_j^{(n)} = f(z_j^{(n)})$\n",
    "    \n",
    "\\begin{equation}\n",
    "\\frac{\\partial C(w,y^{in})}{\\partial w_\\star} = \\sum_j (y_j^{(n)} - F_j(y^{in}))\\frac{\\partial y_j^{(n)}}{\\partial w_\\star}= \\sum_j (y_j^{(n)} - F_j(y^{in})) f'(z_j^{(n)})\\frac{\\partial z_j^{(n)}}{\\partial w_\\star}, \\qquad layer\\quad n\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j^{(n)}}{\\partial w_\\star}= \\sum_k \\frac{\\partial z_j^{(n)}}{\\partial y_k^{(n-1)}}\\frac{\\partial y_k^{(n-1)}}{\\partial w_\\star} = \\sum_k w_{jk}^{n,n-1} f'(z_k^{(n-1)})\\frac{\\partial z_k^{(n-1)}}{\\partial w_\\star}, \\qquad layer\\quad n-1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "z_j^{(n)}=w_{j1}^{(n)}y_{1}^{(n)}+...+w_{jk}^{(n)}y_{k}^{(n)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j^{(n)}}{\\partial w_\\star}= \\sum_k w_{jk}^{n,n-1} f'(z_k^{(n-1)})\\frac{\\partial z_k^{(n-1)}}{\\partial w_\\star} = \\sum_k M_{jk}^{(n,n-1)}\\frac{\\partial z_k^{(n-1)}}{\\partial w_\\star}\n",
    "\\end{equation}\n",
    "\n",
    "as each pair of layers $n, n-1$ contributes multiplication with the $ M_{jk}^{(n,n-1)}$ matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "w_{jk}^{n,n-1} f'(z_k^{(n-1)}) = M_{jk}^{(n,n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "**BACKPROPAGATION:**\n",
    "\n",
    "\\begin{equation}\n",
    "C(w)=<C(w,y^{in})>\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C(w,y^{in})}{\\partial w_\\star} = \\sum_j (y_j^{(n)} - F_j(y^{in})) f'(z_j^{(n)})\\frac{\\partial z_j^{(n)}}{\\partial w_\\star}\n",
    "\\end{equation}\n",
    "\n",
    ">Construct vector for output layer $n$ and multiply with matrices from the right:\n",
    "\n",
    "    (1) Initialize vector from output layer\n",
    "    \n",
    "\\begin{equation}\n",
    "\\Delta_j=(y_j^{(n)} - F_j(y^{in})) f'(z_j^{(n)})\n",
    "\\end{equation}\n",
    "\n",
    "    (2) For each layer, store outcomes -cost derivatives- for all weights and biases $w_\\star$ in that layer:\n",
    "    \n",
    "\\begin{equation}\n",
    "\\frac{\\partial C(w,y^{in})}{\\partial w_\\star} =\\Delta_j \\frac{\\partial z_j^{(n)}}{\\partial w_\\star}\n",
    "\\end{equation}\n",
    "\n",
    "    (3) Multiply \n",
    "    \n",
    "\\begin{equation}\n",
    "\\Delta_k^{new} = \\sum_j \\Delta_j M_{jk}^{(n,n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "**In each layer:**\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C(w,y^{in})}{\\partial w_\\star} =\\Delta_j \\frac{\\partial z_j^{(n)}}{\\partial w_\\star}\n",
    "\\end{equation}\n",
    "\n",
    "Weight:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j^{(n)}}{\\partial w_{jk}^{n,n-1}}= y_k^{(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "Bias:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j}{\\partial b_{j}^{n}}= 1\n",
    "\\end{equation}\n",
    "\n",
    "-Averaging over samples-\n",
    "\n",
    "\\begin{equation}\n",
    "Weight: \\qquad \\frac{\\partial C}{\\partial w_{jk}^{n,n-1}}= <\\Delta_j y_k^{(n-1)}> \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Bias: \\qquad \\frac{\\partial C}{\\partial b_j^n}= <\\Delta_j> \n",
    "\\end{equation}\n",
    "\n",
    "*Store the weights & biases on a list -> matrices of different dimensions depending on the layer*.\n",
    "\n",
    "**Learning rate**, $\\eta$: how far do I go along the gradient of the cost function with respect to the weights. \n",
    "    \n",
    "    - Take grad of cost function with respect to te weights\n",
    "    - Change the weights in this direction\n",
    "  \n",
    "Small batch size + large learning rate = Problematic. \n",
    "\n",
    "\\begin{equation}\n",
    "C(w-\\eta \\nabla_w C) \\approx C(w) - \\eta (\\nabla_w C)(\\nabla_w C) + ... \n",
    "\\end{equation}\n",
    "\n",
    "$C(w-\\eta \\nabla_w C)$, new weights, $\\nabla_w C$ decrease in C (always positive). \n",
    "\n",
    "Problems: step too large (need higher order terms); bad approx. of C (for small batch size, approx. of C fluctuates). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe09c6",
   "metadata": {},
   "source": [
    "##### Backpropagation implementation\n",
    "\n",
    "           VARIABLE - DIMENSIONS:\n",
    "\n",
    "           y[layer] - batchsize x neurons[layer]\n",
    "\n",
    "              delta - batchsize x neurons[layer]\n",
    "\n",
    "     Weights[layer] - neurons[lower layer] x neurons[layer]\n",
    "\n",
    "      Biases[layer] - neurons[layer]\n",
    "      \n",
    "*Store the weights and biases on two lists \"Weights\" and \"Biases\": e.g., for 3 layers (except input)*\n",
    "\n",
    "    Layer 3: Biases[2], Weights[2], y_layer[3], df_layer[2] (stores f'(z))\n",
    "    \n",
    "    Layer 2: Biases[1], Weights[1], y_layer[2], df_layer[1] (stores f'(z))\n",
    "\n",
    "    Layer 1: Biases[0], Weights[0] (2x3 matrix), y_layer[1], df_layer[0] (stores f'(z))\n",
    "   \n",
    "    Layer 0: y_layer[3] (input layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dWeights[layer]=dot(transpose(y[lower layer]),Delta)/batchsize\n",
    "dBiases[layer]=Delta.sum(0)/batchsize\n",
    "\n",
    "Delta=dot(Delta,transpose(Weights))*df_layer[lower layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3895ae9",
   "metadata": {},
   "source": [
    "##### Neural Network: \n",
    "\n",
    "General purpose algorithm: feedforward & backpropagation\n",
    "\n",
    "Problem-specific: \n",
    "\n",
    "    1. Choose network layout - Hyperparameters\n",
    "        # layers\n",
    "        # neurons per layer\n",
    "        Type of non-linear function\n",
    "        Maybe specialized structures for weights/biases\n",
    "    2. Generate training / validation / test samples\n",
    "    3. Monitor / Optimize training process - Hyperparameters\n",
    "        Choose learning rate (eta) & batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f(z) -non-linear function- and f'(z) -its derivative-\n",
    "def net_f_df(z): \n",
    "    val=1/(1+exp(-z))\n",
    "    return(val,exp(-z)*val(**2)) # Returns f and f' which makes it more efficient\n",
    "\n",
    "# FORWARD PROPAGATION:\n",
    "# Calculate the values in next layer. One forward step through the network: goven the old values 'y', \n",
    "# calculate (1) first my linear step 'z' and (2) apply the non-linear function:\n",
    "def forward_step(y,w,b): # w=weights, b=bias in next layer\n",
    "    z=dot(y,w)+b \n",
    "    return(net_f_df(z)) # Apply non-linearity\n",
    "\n",
    "# Function that runs the full network: (1) put the values of the input neurons 'y_in'on the function, \n",
    "# (2) start at the lowest layer 'y=y_in, y_layer[0]=y', (3) range through all the layers from lowest\n",
    "# to top while storing all the values (df, y) and (4) return the highest value 'y'\n",
    "def apply_net(y_in): \n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer\n",
    "    y=y_in\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # Loop through all layers, j=0 corresponds to the 1st layer above input\n",
    "        y,df_forward_step(y,Weights[j],Biases[j])\n",
    "        df_layer[j]=df # Store f'(z)\n",
    "        y_layer[j+1]=y # Store f(z)\n",
    "    return(y)\n",
    "# We store all the values df and y because we will need for backpropagation as it depends on things like f'(z), i.e., df(z),\n",
    "# at the corresponding layer; these are the z's I've calculated before during the forward propagation.\n",
    "\n",
    "# BACKWARD PROPAGATION:\n",
    "# Delta at layer N, of batchsize x layersize(N); w[layersize(N-1) x layersize(N) matrix]; \n",
    "# df=df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "def backward_step(delta,w,df):\n",
    "    return(dot(delta,transpose(w))*df) # Deviation vector (delta) multiplied by the weight matrix w and f'(z)\n",
    "\n",
    "# One backward pass; result will be 'dw_layer' matrices with the derivatives of the \n",
    "# cost fuction with respect to the corresponding weight (similar for biases).\n",
    "# (1) I tell the algorithm what whould have been the correct value (y_target)\n",
    "# (2) Calculate the deviation vector 'delta' which is essentially the difference between the correct value (y_target)\n",
    "# and the actual output (y_layer)\n",
    "# (3) Range through all the layers\n",
    "# (4) Collect all the gradients of my cost function with respect to the weights of the layer (dw_layer) \n",
    "# -similar for bias, db_layer -\n",
    "def backdrop(y_target):\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1] # Index [-1] means the LAST index (i.e., the higher layer)\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1): # Range through all the layers\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)/batchsize\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1ca30",
   "metadata": {},
   "source": [
    "### (1) Implement backpropagation: example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace, transpose\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib \n",
    "matplotlib.rcParams['figure.dpi']=100 # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d8c34",
   "metadata": {},
   "source": [
    "##### Define the function that applies the non-linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=1/(1+exp(-z)) # sigmoid\n",
    "    return(val,exp(-z)*(val**2)) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cfc1",
   "metadata": {},
   "source": [
    "##### Define the function that applies 1 forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2aeb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(y,w,b): # calculate values in next layer, from input y\n",
    "    z=dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z)) # apply nonlinearity and return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c9c0d",
   "metadata": {},
   "source": [
    "##### Define the function that goes forward through all the layers from input to output: given the input values, outputs the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net(y_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "        df_layer[j]=df # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=y # store f(z) [also needed in backprop]        \n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net_simple(y_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "\n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe135ff",
   "metadata": {},
   "source": [
    "##### Define the function that applies 1 step of backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return(dot(delta,transpose(w))*df )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444d571",
   "metadata": {},
   "source": [
    "##### Define the function that applies the full backpropagation: stored values from forward pass are used in backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d459b1",
   "metadata": {},
   "source": [
    "##### Gradient step: takes the gradients that have been calculated by the backpropagation (i.e., dw_layer) and changes the weights of the layer in the direction of the negative gradient.  Learning rate (eta) tells me how large are the steps -should not be too large neither too small - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28461de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d41e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_in,y_target,eta): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(y_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=((y_target-y_out_result)**2).sum()/batchsize\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48254ba8",
   "metadata": {},
   "source": [
    "##### Train the net to reproduce a 2D function - Produce random batches: randomly sample a function defined on a 2D square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82672a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up rectified linear units (relu) instead of sigmoid\n",
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=z*(z>0)\n",
    "    return(val,z>0) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8d7d1",
   "metadata": {},
   "source": [
    "##### Weights are defined from a random function (in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d31d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f56c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batchsize\n",
    "batchsize=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452c6dc",
   "metadata": {},
   "source": [
    "##### Define the function we want to have (desired outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a change: Set up rectified linear units (relu) \n",
    "# instead of sigmoid\n",
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=z*(z>0)\n",
    "    return(val,z>0) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073a6cb",
   "metadata": {},
   "source": [
    "##### Pick batchsize random positions in 2D square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    global batchsize\n",
    "\n",
    "    inputs=random.uniform(low=-0.5,high=+0.5,size=[batchsize,2])\n",
    "    targets=zeros([batchsize,1]) # must have right dimensions\n",
    "    targets[:,0]=myFunc(inputs[:,0],inputs[:,1])\n",
    "    return(inputs,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21896f",
   "metadata": {},
   "source": [
    "##### Do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78728412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on randomly sampled points to make the network reproduce better and better this 2D function!\n",
    "eta=.001\n",
    "batches=2000\n",
    "costs=zeros(batches)\n",
    "\n",
    "for k in range(batches):\n",
    "    y_in,y_target=make_batch()\n",
    "    costs[k]=train_net(y_in,y_target,eta)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.title(\"Cost function during training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to evaluate the (randomly initialized) network\n",
    "# on some area in the 2D plane\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "testoutput=apply_net_simple(testsample)\n",
    "myim=plt.imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fec2d",
   "metadata": {},
   "source": [
    "##### Animate the NN results during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fresh:\n",
    "\n",
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for updating display \n",
    "# (simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "eta=0.01 # learning rate\n",
    "nsteps=100\n",
    "\n",
    "costs=zeros(nsteps)\n",
    "for j in range(nsteps):\n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4)) # prepare figure\n",
    "    ax[1].axis('off') # no axes\n",
    "    \n",
    "    # the crucial lines:\n",
    "    y_in,y_target=make_batch() # random samples (points in 2D)\n",
    "    costs[j]=train_net(y_in,y_target,eta) # train network (one step, on this batch)\n",
    "    testoutput=apply_net_simple(testsample) # check the new network output in the plane\n",
    "    \n",
    "    img=ax[1].imshow(reshape(testoutput,shape(X0)),interpolation='nearest',origin='lower') # plot image\n",
    "    ax[0].plot(costs)\n",
    "    \n",
    "    ax[0].set_title(\"Cost during training\")\n",
    "    ax[0].set_xlabel(\"number of batches\")\n",
    "    ax[1].set_title(\"Current network prediction\")\n",
    "    plt.show()\n",
    "    sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
