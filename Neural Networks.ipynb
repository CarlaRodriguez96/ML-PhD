{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da266d54",
   "metadata": {},
   "source": [
    "## Neural Networks: smol guide & examples\n",
    "\n",
    "from https://pad.gwdg.de/s/Machine_Learning_For_Physicists_2021#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5ad27",
   "metadata": {},
   "source": [
    "### Basic NN (no hidden layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import * \n",
    "\n",
    "N0=3 #Input layer size\n",
    "N1=2 #Output layer size\n",
    "\n",
    "w=random.uniform(low=-1, high=+1, size=(N1,N0)) #Weights\n",
    "b=random.uniform(low=-1, high=+1, size=N1) #Bias\n",
    "\n",
    "y_in=array([0.2, 0.4, -0.1]) #Input layer of values\n",
    "\n",
    "z=dot(w,y_in)+b #Weighted sum of input values\n",
    "\n",
    "y_out=1/(1+exp(-z)) #Sigmoid function (activate function)\n",
    "\n",
    "def apply_net(y_in):\n",
    "    global w,b\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9141ad7",
   "metadata": {},
   "source": [
    "### Multilayer NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f446eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the operation of a layer\n",
    "def apply_layer(y_in,w,b):  #Weights and biases are no longer global, they change from layer to layer\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))\n",
    "\n",
    "N0=2 #Input layer size\n",
    "N1=30 #Hidden layer size\n",
    "N2=1 #Output layer size\n",
    "\n",
    "#FROM INPUT LAYER TO HIDDEN LAYER:\n",
    "w1=random.uniform(low=-10, high=+10, size=(N1,N0)) #Weights, size is always (output dimension x input dimension)\n",
    "b1=random.uniform(low=-1, high=+1, size=N1) #Bias\n",
    "\n",
    "#FROM HIDDEN LAYER TO OUTPUT LAYER:\n",
    "w2=random.uniform(low=-10, high=+10, size=(N2,N1)) #Weights\n",
    "b2=random.uniform(low=-1, high=+1, size=N2) #Bias\n",
    "\n",
    "#Define the network: succession of layers\n",
    "def apply_net(y_in):\n",
    "    global w1,b1,w2,b2\n",
    "    \n",
    "    y1=apply_layer(y_in,w1,b1)\n",
    "    y2=apply_layer(y1,w2,b2)\n",
    "    return(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad816e",
   "metadata": {},
   "source": [
    "### NN with many hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed67650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the operation of a layer\n",
    "def apply_layer(y_in,w,b):  #Weights and biases are no longer global, they change from layer to layer\n",
    "    \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))\n",
    "\n",
    "Nlayers=20 \n",
    "LayerSize=100\n",
    "\n",
    "Weights=random.uniform(low=-3, high=3, size=[Nlayers,LayerSize,LayerSize])\n",
    "Biases=random.uniform(low=-1, high=1, size=[Nlayers,LayerSize])\n",
    "\n",
    "#First hidden layer (coming in from the input layer)\n",
    "WeightsFirst=random.uniform(low=-1, high=+1, size=[2,LayerSize]) \n",
    "BiasesFirst=random.uniform(low=-1, high=+1, size=LayerSize) \n",
    "\n",
    "#Final layer (output neuron)\n",
    "WeightsFinal=random.uniform(low=-1, high=+1, size=[LayerSize,1]) \n",
    "BiasesFinal=random.uniform(low=-1, high=+1, size=1) \n",
    "\n",
    "#Define the network: succession of layers\n",
    "def apply_multi_net(y_in):\n",
    "    global Weights, Biases, WeightsFinal, BiasesFinal, Nlayers\n",
    "    \n",
    "    y=apply_layer(y_in,WeightsFirst,BiasesFirst)\n",
    "    for j in range(Nlayers):\n",
    "        y=apply_layer(y,Weights[j,:,:],Biases[j,:])\n",
    "        \n",
    "    output=apply_layer(y,WeigthsFinal,BiasesFinal)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe09c6",
   "metadata": {},
   "source": [
    "### Backpropagation implementation\n",
    "\n",
    "           VARIABLE - DIMENSIONS:\n",
    "\n",
    "           y[layer] - batchsize x neurons[layer]\n",
    "\n",
    "              delta - batchsize x neurons[layer]\n",
    "\n",
    "     Weights[layer] - neurons[lower layer] x neurons[layer]\n",
    "\n",
    "      Biases[layer] - neurons[layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dWeights[layer]=dot(transpose(y[lower layer]),Delta)/batchsize\n",
    "dBiases[layer]=Delta.sum(0)/batchsize\n",
    "\n",
    "Delta=dot(Delta,transpose(Weights))*df_layer[lower layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e91ec5",
   "metadata": {},
   "source": [
    "Store the weights and biases on two lists \"Weights\" and \"Biases\":\n",
    "\n",
    "e.g., for 3 layers (except input)\n",
    "\n",
    "    Layer 3: Biases[2], Weights[2], y_layer[3], df_layer[2] (stores f'(z))\n",
    "    \n",
    "    Layer 2: Biases[1], Weights[1], y_layer[2], df_layer[1] (stores f'(z))\n",
    "\n",
    "    Layer 1: Biases[0], Weights[0] (2x3 matrix), y_layer[1], df_layer[0] (stores f'(z))\n",
    "   \n",
    "    Layer 0: y_layer[3] (input layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277998f",
   "metadata": {},
   "source": [
    "### Neural Network: \n",
    "\n",
    "General purpose algorithm: feedforward & backpropagation\n",
    "\n",
    "Problem-specific: \n",
    "\n",
    "    1. Choose network layout - Hyperparameters\n",
    "        # layers\n",
    "        # neurons per layer\n",
    "        Type of non-linear function\n",
    "        Maybe specialized structures for weights/biases\n",
    "    2. Generate training / validation / test samples\n",
    "    3. Monitor / Optimize training process - Hyperparameters\n",
    "        Choose learning rate (eta) & batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f(z) -non-linear function- and f'(z) -its derivative-\n",
    "def net_f_df(z): \n",
    "    val=1/(1+exp(-z))\n",
    "    return(val,exp(-z)*val(**2)) # Returns f and f' which makes it more efficient\n",
    "\n",
    "# FORWARD PROPAGATION:\n",
    "# Calculate the values in next layer. One forward step through the network: goven the old values 'y', \n",
    "# calculate (1) first my linear step 'z' and (2) apply the non-linear function:\n",
    "def forward_step(y,w,b): # w=weights, b=bias in next layer\n",
    "    z=dot(y,w)+b \n",
    "    return(net_f_df(z)) # Apply non-linearity\n",
    "\n",
    "# Function that runs the full network: (1) put the values of the input neurons 'y_in'on the function, \n",
    "# (2) start at the lowest layer 'y=y_in, y_layer[0]=y', (3) range through all the layers from lowest\n",
    "# to top while storing all the values (df, y) and (4) return the highest value 'y'\n",
    "def apply_net(y_in): \n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer\n",
    "    y=y_in\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # Loop through all layers, j=0 corresponds to the 1st layer above input\n",
    "        y,df_forward_step(y,Weights[j],Biases[j])\n",
    "        df_layer[j]=df # Store f'(z)\n",
    "        y_layer[j+1]=y # Store f(z)\n",
    "    return(y)\n",
    "# We store all the values df and y because we will need for backpropagation as it depends on things like f'(z), i.e., df(z),\n",
    "# at the corresponding layer; these are the z's I've calculated before during the forward propagation.\n",
    "\n",
    "# BACKWARD PROPAGATION:\n",
    "# Delta at layer N, of batchsize x layersize(N); w[layersize(N-1) x layersize(N) matrix]; \n",
    "# df=df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "def backward_step(delta,w,df):\n",
    "    return(dot(delta,transpose(w))*df) # Deviation vector (delta) multiplied by the weight matrix w and f'(z)\n",
    "\n",
    "# One backward pass; result will be 'dw_layer' matrices with the derivatives of the \n",
    "# cost fuction with respect to the corresponding weight (similar for biases).\n",
    "# (1) I tell the algorithm what whould have been the correct value (y_target)\n",
    "# (2) Calculate the deviation vector 'delta' which is essentially the difference between the correct value (y_target)\n",
    "# and the actual output (y_layer)\n",
    "# (3) Range through all the layers\n",
    "# (4) Collect all the gradients of my cost function with respect to the weights of the layer (dw_layer) \n",
    "# -similar for bias, db_layer -\n",
    "def backdrop(y_target):\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1] # Index [-1] means the LAST index (i.e., the higher layer)\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1): # Range through all the layers\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)/batchsize\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1ca30",
   "metadata": {},
   "source": [
    "### (1) Implement backpropagation: example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace, transpose\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib \n",
    "matplotlib.rcParams['figure.dpi']=100 # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d8c34",
   "metadata": {},
   "source": [
    "##### Define the function that applies the non-linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=1/(1+exp(-z)) # sigmoid\n",
    "    return(val,exp(-z)*(val**2)) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cfc1",
   "metadata": {},
   "source": [
    "##### Define the function that applies 1 forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2aeb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(y,w,b): # calculate values in next layer, from input y\n",
    "    z=dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z)) # apply nonlinearity and return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c9c0d",
   "metadata": {},
   "source": [
    "##### Define the function that goes forward through all the layers from input to output: given the input values, outputs the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net(y_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "        df_layer[j]=df # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=y # store f(z) [also needed in backprop]        \n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net_simple(y_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "\n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe135ff",
   "metadata": {},
   "source": [
    "##### Define the function that applies 1 step of backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return(dot(delta,transpose(w))*df )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444d571",
   "metadata": {},
   "source": [
    "##### Define the function that applies the full backpropagation: stored values from forward pass are used in backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d459b1",
   "metadata": {},
   "source": [
    "##### Gradient step: takes the gradients that have been calculated by the backpropagation (i.e., dw_layer) and changes the weights of the layer in the direction of the negative gradient.  Learning rate (eta) tells me how large are the steps -should not be too large neither too small - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28461de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d41e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_in,y_target,eta): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(y_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=((y_target-y_out_result)**2).sum()/batchsize\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874af66d",
   "metadata": {},
   "source": [
    "##### Train the net to reproduce a 2D function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48254ba8",
   "metadata": {},
   "source": [
    "##### Train the net to reproduce a 2D function - Produce random batches: randomly sample a function defined on a 2D square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82672a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up rectified linear units (relu) instead of sigmoid\n",
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=z*(z>0)\n",
    "    return(val,z>0) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8d7d1",
   "metadata": {},
   "source": [
    "##### Weights are defined from a random function (in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d31d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f56c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batchsize\n",
    "batchsize=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452c6dc",
   "metadata": {},
   "source": [
    "##### Define the function we want to have (desired outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a change: Set up rectified linear units (relu) \n",
    "# instead of sigmoid\n",
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=z*(z>0)\n",
    "    return(val,z>0) # return both f and f'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073a6cb",
   "metadata": {},
   "source": [
    "##### Pick batchsize random positions in 2D square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    global batchsize\n",
    "\n",
    "    inputs=random.uniform(low=-0.5,high=+0.5,size=[batchsize,2])\n",
    "    targets=zeros([batchsize,1]) # must have right dimensions\n",
    "    targets[:,0]=myFunc(inputs[:,0],inputs[:,1])\n",
    "    return(inputs,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21896f",
   "metadata": {},
   "source": [
    "##### Do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78728412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on randomly sampled points to make the network reproduce better and better this 2D function!\n",
    "eta=.001\n",
    "batches=2000\n",
    "costs=zeros(batches)\n",
    "\n",
    "for k in range(batches):\n",
    "    y_in,y_target=make_batch()\n",
    "    costs[k]=train_net(y_in,y_target,eta)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.title(\"Cost function during training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to evaluate the (randomly initialized) network\n",
    "# on some area in the 2D plane\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "testoutput=apply_net_simple(testsample)\n",
    "myim=plt.imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fec2d",
   "metadata": {},
   "source": [
    "#### Animate the NN results during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fresh:\n",
    "\n",
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for updating display \n",
    "# (simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "eta=0.01 # learning rate\n",
    "nsteps=100\n",
    "\n",
    "costs=zeros(nsteps)\n",
    "for j in range(nsteps):\n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4)) # prepare figure\n",
    "    ax[1].axis('off') # no axes\n",
    "    \n",
    "    # the crucial lines:\n",
    "    y_in,y_target=make_batch() # random samples (points in 2D)\n",
    "    costs[j]=train_net(y_in,y_target,eta) # train network (one step, on this batch)\n",
    "    testoutput=apply_net_simple(testsample) # check the new network output in the plane\n",
    "    \n",
    "    img=ax[1].imshow(reshape(testoutput,shape(X0)),interpolation='nearest',origin='lower') # plot image\n",
    "    ax[0].plot(costs)\n",
    "    \n",
    "    ax[0].set_title(\"Cost during training\")\n",
    "    ax[0].set_xlabel(\"number of batches\")\n",
    "    ax[1].set_title(\"Current network prediction\")\n",
    "    plt.show()\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee90471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
